{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from typing import Tuple\n",
    "import yaml\n",
    "from naive_regression.crypto_utils import setup_crypto\n",
    "from naive_regression.ematrix import EMatrix\n",
    "import numpy as np\n",
    "\n",
    "from naive_regression.np_reference import train\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54abaeba",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "## Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac07a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(prediction: EMatrix, label: EMatrix,\n",
    "                   inverse_num_samples_scale: float,\n",
    "                   ) -> Tuple[EMatrix, EMatrix]:\n",
    "    residuals = label - prediction\n",
    "    # compute error (difference between estimate y_hat and true value y)\n",
    "    sq_error = residuals.hprod(residuals)\n",
    "    enc_SSE = sq_error.sum()\n",
    "\n",
    "    enc_SSE *= inverse_num_samples_scale\n",
    "    return residuals, enc_SSE\n",
    "\n",
    "\n",
    "def apply_gradient(\n",
    "        X: EMatrix,\n",
    "        weights: EMatrix,\n",
    "        residuals: EMatrix,\n",
    "        scaling: float,\n",
    "        alpha: float,\n",
    "        repeat_weights_N_times: int,\n",
    ") -> Tuple[EMatrix, EMatrix]:\n",
    "    \"\"\"\n",
    "    We return the new weights and the gradients to generate these weights\n",
    "        this is to allow us to inspect if we need to.\n",
    "    \"\"\"\n",
    "    # Internally, the dot product handles the need for the transpose.\n",
    "\n",
    "    grad = X.dot(residuals, \"vertical\")\n",
    "    grad = grad * -2 * scaling\n",
    "\n",
    "    grad_alpha = grad * alpha\n",
    "    repeated_grad_alpha = grad_alpha.vecConv2Hrep(repeat_weights_N_times)\n",
    "    weights = weights - repeated_grad_alpha\n",
    "    return weights, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace7934",
   "metadata": {},
   "source": [
    "## Exercise Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef44fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(\n",
    "        X:EMatrix,\n",
    "        weights: EMatrix,\n",
    ") -> EMatrix:\n",
    "    ################################################\n",
    "    # Exe: implement the prediction via. a dot-product.\n",
    "    #       think carefully about what the out-packing might be\n",
    "    ################################################\n",
    "    return X.dot(weights, \"vertical\")\n",
    "\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda605db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"naive_regression/config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(\"ML Config:\")\n",
    "pprint(config[\"ml_params\"])\n",
    "print(\"Crypto Params:\")\n",
    "pprint(config[\"crypto_params\"])\n",
    "if config[\"crypto_params\"][\"run_bootstrap\"]:\n",
    "    print(\"Running with bootstrap\")\n",
    "    pprint(config[\"crypto_bootstrap_params\"])\n",
    "ml_conf = config[\"ml_params\"]\n",
    "batch_size = ml_conf[\"batch_size\"]\n",
    "lr = ml_conf[\"lr\"]\n",
    "epochs = ml_conf[\"epochs\"]\n",
    "\n",
    "################################################\n",
    "# Generate data\n",
    "################################################\n",
    "\n",
    "X = np.random.rand(batch_size * 5, 5)\n",
    "y = (np.dot(X, np.random.rand(5, 1))) + np.random.rand(1)\n",
    "noise = np.random.randn(y.shape[0], y.shape[1])\n",
    "y = y + noise\n",
    "\n",
    "weights = np.random.rand(5, 1)\n",
    "print(\"#\" * 10)\n",
    "print(\"Plaintext Performance\")\n",
    "m_stat = train(X, y, weights, lr, epochs)\n",
    "\n",
    "print(\"#\" * 10)\n",
    "print(\"Encrypted Performance\")\n",
    "\n",
    "setup_crypto(\n",
    "    num_data_points=-1 if config[\"crypto_params\"][\"run_bootstrap\"] else len(X),\n",
    "    c_params=config[\"crypto_params\"],\n",
    "    bootstrap_params=config[\"crypto_bootstrap_params\"]\n",
    ")\n",
    "\n",
    "inverse_scale = 1 / len(y)\n",
    "\n",
    "####################################################################\n",
    "# We need to repeat the weights N-times bc we do the hadamard product then sum\n",
    "#   when we're doing the dot product\n",
    "weights = np.squeeze(weights, axis=1).tolist()\n",
    "repeated_weights = []\n",
    "for i in range(len(X)):\n",
    "    repeated_weights.append(weights)\n",
    "weights = EMatrix.fromList(repeated_weights, packing=\"vertical\", repeated=True)\n",
    "weights.encryptSelf()\n",
    "\n",
    "####################################################################\n",
    "# We encrypt all at once. NOTE: this is not a true SGD - we're not shuffling\n",
    "#   between each epoch. Having said that, this is WAY faster\n",
    "e_X = EMatrix.fromList(X.tolist())\n",
    "e_y = EMatrix.fromList(y.tolist())\n",
    "e_X.encryptSelf()\n",
    "e_y.encryptSelf()\n",
    "run_bootstrap_mode = config[\"crypto_params\"][\"run_bootstrap\"]\n",
    "for epoch in range(epochs):\n",
    "    y_pred = predict(e_X, weights)\n",
    "    if y_pred is None:\n",
    "        raise Exception(\"You have not implemented the predict function yet\")\n",
    "\n",
    "    residuals, loss = calculate_loss(y_pred, label=e_y, inverse_num_samples_scale=inverse_scale)\n",
    "    weights, grads = apply_gradient(e_X, weights, residuals, inverse_scale, lr, len(X))\n",
    "\n",
    "    ################################################\n",
    "    # Exe: it's not always realistic, but you may wish to display the loss\n",
    "    ################################################\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    # Exe: Our ciphertexts accumulate noise as we do computations. We have two options to handle the noise:\n",
    "    #   - bootstrapping, which is expensive\n",
    "    #   - decrypting and re-encrypting, which comes with its own tradeoffs\n",
    "    #   Benchmark the two to get a feel for the timing difference\n",
    "\n",
    "    # weights.bootstrap_self()\n",
    "    # weights.recrypt_self()\n",
    "    ################################################\n",
    "    raise NotImplementedError(\"You'll want to implement the ciphertext noise reduction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876164d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
