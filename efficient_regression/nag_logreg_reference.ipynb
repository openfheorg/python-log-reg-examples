{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bc0082",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "\n",
    "- provide a plaintext interface to analyze step-by-step what is happening in the encrypted code\n",
    "\n",
    "- Used as a Python sanity check because I'm not that familiar with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91303fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T00:30:25.942588365Z",
     "start_time": "2024-01-26T00:30:25.899041257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8a827",
   "metadata": {},
   "source": [
    "# Load and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e34725-f22a-4ce4-bc21-3967727b0d98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T00:30:26.379785141Z",
     "start_time": "2024-01-26T00:30:26.378637249Z"
    }
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def load_data(num_samples, compare_to_r_ref):\n",
    "    x_file = \"../data/X_norm_1024.csv\"\n",
    "    y_file = \"../data/y_1024.csv\"\n",
    "    train_x = pd.read_csv(x_file)\n",
    "    train_x = train_x.to_numpy()[:num_samples]\n",
    "    train_y = pd.read_csv(y_file)\n",
    "    train_y = train_y.to_numpy()[:num_samples]\n",
    "    print(f\"{bcolors.OKGREEN}Using subsampled data to compare Python-C++{bcolors.ENDC}\")\n",
    "    print(f\"{bcolors.OKGREEN}Reading in {x_file}, {y_file} {bcolors.ENDC}\")\n",
    "\n",
    "    print(f\"Train X shape is: {train_x.shape}\")\n",
    "    print(f\"Train y shape is: {train_y.shape}\")\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce32218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T00:30:26.869926870Z",
     "start_time": "2024-01-26T00:30:26.865138540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mUsing subsampled data to compare Python-C++\u001B[0m\n",
      "\u001B[92mReading in ../data/X_norm_1024.csv, ../data/y_1024.csv \u001B[0m\n",
      "Train X shape is: (1024, 10)\n",
      "Train y shape is: (1024, 1)\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 1024\n",
    "COMPARE_TO_R_REF = False\n",
    "lr = 0.1\n",
    "mu = 0.1\n",
    "train_x, train_y = load_data(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    compare_to_r_ref=COMPARE_TO_R_REF\n",
    ")\n",
    "\n",
    "# Same shape as Marcelo's reference code\n",
    "betas = np.zeros((10, ))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def fwd(train_x, betas, dbg=False):\n",
    "    preds = train_x @ betas\n",
    "    return np.expand_dims(sigmoid(preds), -1)\n",
    "\n",
    "def calculate_gradient(train_x, train_y, betas, fwd, dbg):\n",
    "    preds = fwd(train_x, betas, dbg)\n",
    "    gradient = -train_x.T @ (train_y - preds) / len(train_y)\n",
    "    return gradient\n",
    "\n",
    "def cost(x, y, theta):\n",
    "    m = x.shape[0]\n",
    "    h = sigmoid(np.matmul(x, theta))\n",
    "    t1 = np.matmul(-y.T, np.log(h))\n",
    "    t2_a = (1 - y.T)\n",
    "    t2_b = np.log(np.clip(1 - h, 0.000000000000001, np.max(1 - h)))  # Used to get numerical issues\n",
    "    t2 = np.matmul(t2_a, t2_b)\n",
    "\n",
    "    return ((t1 - t2) / m)[0]\n",
    "\n",
    "def nesterov(betas, epochs, lr, mu, train_x, train_y):\n",
    "    import copy\n",
    "\n",
    "    phi = copy.deepcopy(betas)\n",
    "    theta = copy.deepcopy(betas)\n",
    "\n",
    "    nesterov_loss = [0 for _ in range(epochs)]\n",
    "    # for i in tqdm.trange(epochs):\n",
    "    for i in range(epochs):\n",
    "        gradient = calculate_gradient(train_x, train_y, theta, fwd, dbg=False)\n",
    "        phi_prime = theta - lr * np.squeeze(gradient)\n",
    "        if i == 0:\n",
    "            theta = phi_prime\n",
    "        else:\n",
    "            theta = phi_prime + mu * (phi_prime - phi)\n",
    "        phi = phi_prime\n",
    "        loss = cost(train_x, train_y, theta)\n",
    "        if DEBUG:\n",
    "            print(f\"Grad: {(gradient.squeeze() * lr).tolist()}\")\n",
    "            print(f\"Theta: {theta.tolist()}\")\n",
    "            print(f\"Phi: {phi.tolist()}\")\n",
    "        print(f\"Iteration: {i} Loss: {loss}\")\n",
    "        nesterov_loss[i] = loss\n",
    "\n",
    "        # print(f\"New loss: {cost(train_x, train_y, v)[0]}\")\n",
    "    return nesterov_loss, theta, phi\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T00:30:39.988016971Z",
     "start_time": "2024-01-26T00:30:39.941130668Z"
    }
   },
   "id": "9ac264a4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74a3d18-2a23-46e6-897a-2dc859beb572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T00:30:40.619060212Z",
     "start_time": "2024-01-26T00:30:40.613587225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 0.6824088332753198\n",
      "Iteration: 1 Loss: 0.6713903103516063\n",
      "Iteration: 2 Loss: 0.6610257231078792\n",
      "Iteration: 3 Loss: 0.6513242754752047\n",
      "Iteration: 4 Loss: 0.6422209559690284\n",
      "Iteration: 5 Loss: 0.633655276525783\n",
      "Iteration: 6 Loss: 0.6255762093407051\n",
      "Iteration: 7 Loss: 0.6179406479255354\n",
      "Iteration: 8 Loss: 0.6107117365232372\n",
      "Iteration: 9 Loss: 0.6038575610703887\n",
      "Iteration: 10 Loss: 0.5973501566062397\n",
      "Iteration: 11 Loss: 0.5911647557546322\n",
      "Iteration: 12 Loss: 0.5852792172730221\n",
      "Iteration: 13 Loss: 0.5796735885054826\n",
      "Iteration: 14 Loss: 0.5743297673054131\n",
      "Iteration: 15 Loss: 0.5692312379069118\n",
      "Iteration: 16 Loss: 0.564362861908849\n",
      "Iteration: 17 Loss: 0.559710710508802\n",
      "Iteration: 18 Loss: 0.555261927800757\n",
      "Iteration: 19 Loss: 0.5510046176555508\n",
      "Iteration: 20 Loss: 0.5469277486855726\n",
      "Iteration: 21 Loss: 0.5430210732440103\n",
      "Iteration: 22 Loss: 0.5392750574653908\n",
      "Iteration: 23 Loss: 0.5356808201235694\n",
      "Iteration: 24 Loss: 0.5322300786434356\n",
      "Iteration: 25 Loss: 0.5289151010105486\n",
      "Iteration: 26 Loss: 0.5257286626205062\n",
      "Iteration: 27 Loss: 0.5226640073275316\n",
      "Iteration: 28 Loss: 0.5197148121116633\n",
      "Iteration: 29 Loss: 0.516875154902066\n",
      "Iteration: 30 Loss: 0.5141394851819239\n",
      "Iteration: 31 Loss: 0.5115025970664695\n",
      "Iteration: 32 Loss: 0.5089596045959669\n",
      "Iteration: 33 Loss: 0.5065059190241907\n",
      "Iteration: 34 Loss: 0.5041372279132745\n",
      "Iteration: 35 Loss: 0.5018494758699237\n",
      "Iteration: 36 Loss: 0.4996388467775385\n",
      "Iteration: 37 Loss: 0.4975017473948975\n",
      "Iteration: 38 Loss: 0.4954347922055596\n",
      "Iteration: 39 Loss: 0.49343478941364477\n",
      "Iteration: 40 Loss: 0.4914987279915835\n",
      "Iteration: 41 Loss: 0.48962376569411786\n",
      "Iteration: 42 Loss: 0.48780721796050297\n",
      "Iteration: 43 Loss: 0.4860465476337059\n",
      "Iteration: 44 Loss: 0.4843393554315164\n",
      "Iteration: 45 Loss: 0.48268337111003146\n",
      "Iteration: 46 Loss: 0.4810764452649769\n",
      "Iteration: 47 Loss: 0.4795165417208774\n",
      "Iteration: 48 Loss: 0.4780017304622418\n",
      "Iteration: 49 Loss: 0.4765301810646989\n",
      "Iteration: 50 Loss: 0.47510015658748345\n",
      "Iteration: 51 Loss: 0.4737100078918236\n",
      "Iteration: 52 Loss: 0.47235816835267386\n",
      "Iteration: 53 Loss: 0.4710431489338816\n",
      "Iteration: 54 Loss: 0.4697635335992973\n",
      "Iteration: 55 Loss: 0.468517975034558\n",
      "Iteration: 56 Loss: 0.4673051906563069\n",
      "Iteration: 57 Loss: 0.4661239588874728\n",
      "Iteration: 58 Loss: 0.4649731156789402\n",
      "Iteration: 59 Loss: 0.4638515512595086\n",
      "Iteration: 60 Loss: 0.4627582070974713\n",
      "Iteration: 61 Loss: 0.46169207305845594\n",
      "Iteration: 62 Loss: 0.46065218474538483\n",
      "Iteration: 63 Loss: 0.4596376210075059\n",
      "Iteration: 64 Loss: 0.4586475016064723\n",
      "Iteration: 65 Loss: 0.4576809850283719\n",
      "Iteration: 66 Loss: 0.45673726643146506\n",
      "Iteration: 67 Loss: 0.45581557572017406\n",
      "Iteration: 68 Loss: 0.454915175736588\n",
      "Iteration: 69 Loss: 0.454035360561409\n",
      "Iteration: 70 Loss: 0.45317545391687253\n",
      "Iteration: 71 Loss: 0.4523348076647369\n",
      "Iteration: 72 Loss: 0.4515128003929474\n",
      "Iteration: 73 Loss: 0.45070883608505846\n",
      "Iteration: 74 Loss: 0.44992234286692695\n",
      "Iteration: 75 Loss: 0.44915277182559643\n",
      "Iteration: 76 Loss: 0.44839959589565664\n",
      "Iteration: 77 Loss: 0.4476623088087063\n",
      "Iteration: 78 Loss: 0.4469404241018604\n",
      "Iteration: 79 Loss: 0.44623347418152765\n",
      "Iteration: 80 Loss: 0.4455410094389602\n",
      "Iteration: 81 Loss: 0.444862597414315\n",
      "Iteration: 82 Loss: 0.44419782200619884\n",
      "Iteration: 83 Loss: 0.44354628272387786\n",
      "Iteration: 84 Loss: 0.44290759397952667\n",
      "Iteration: 85 Loss: 0.44228138441807263\n",
      "Iteration: 86 Loss: 0.441667296282357\n",
      "Iteration: 87 Loss: 0.44106498481148704\n",
      "Iteration: 88 Loss: 0.44047411767039835\n",
      "Iteration: 89 Loss: 0.43989437440877754\n",
      "Iteration: 90 Loss: 0.43932544594761747\n",
      "Iteration: 91 Loss: 0.4387670340917911\n",
      "Iteration: 92 Loss: 0.4382188510671364\n",
      "Iteration: 93 Loss: 0.43768061908064104\n",
      "Iteration: 94 Loss: 0.43715206990240707\n",
      "Iteration: 95 Loss: 0.4366329444681621\n",
      "Iteration: 96 Loss: 0.436122992501158\n",
      "Iteration: 97 Loss: 0.4356219721523744\n",
      "Iteration: 98 Loss: 0.43512964965801304\n",
      "Iteration: 99 Loss: 0.4346457990133267\n"
     ]
    }
   ],
   "source": [
    "losses, theta, phi = nesterov(betas, 100, lr, mu, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745a6998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T00:29:28.619907319Z",
     "start_time": "2024-01-26T00:29:28.618874967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[0.6824088332753198, 0.6713903103516063, 0.6610257231078792]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[:10]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "0.4697637653026297\n",
    "0.4697635335992973"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T00:29:28.620058194Z",
     "start_time": "2024-01-26T00:29:28.618923800Z"
    }
   },
   "id": "26de4e30ae1a77",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
